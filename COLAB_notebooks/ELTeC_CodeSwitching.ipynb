{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELTeC_CodeSwitching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhwfIKZ8OuhwHVPL076kLB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bncolorado/Processing-ELTeC-corpus/blob/main/COLAB_notebooks/ELTeC_CodeSwitching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01-XgVe2EIAP"
      },
      "source": [
        "# Extracting code switching\n",
        "\n",
        "One of the TEI tags used in ELTeC corpus is \"foreign\", that marks code switching. This notebook shows how to extract the languages other than Spanish (castilian) in ELTeC-SPA and to generates a word cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZO3HPMGHDe"
      },
      "source": [
        "## Loading ELTeC-SPA corpus in Colab\n",
        "\n",
        "Only ELTeC-SPA will be loaded.\n",
        "\n",
        "Corpus URL is https://github.com/COST-ELTeC/ELTeC-spa > \"code\" > copy \"Download ZIP\"\n",
        "\n",
        "To load other collection (other languages): https://github.com/COST-ELTeC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyLyrpBnD6v2"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget \"https://github.com/COST-ELTeC/ELTeC-spa/archive/refs/heads/master.zip\" # paste here corpus url / Copiar aquíí la url del corpus\n",
        "\n",
        "zip_ref = zipfile.ZipFile('master.zip', 'r') #Opens the zip file in \"read\" mode / abre el fichero comprimido en modo lectura\n",
        "zip_ref.extractall() #Extracts files here (/content/) / Extrae los ficheros en la carpeta \"content\" (la actual)\n",
        "zip_ref.close() #Cierro fichero\n",
        "!rm master.zip #Removes ZIP to save space / Elimina el ZIP para ganar espacio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47nMNl6ZUlXt"
      },
      "source": [
        "Now, the novels with the XML-TEI annotation are in this directory:\n",
        "\n",
        "/content/ELTeC-spa-master/level1/\n",
        "\n",
        "It is the level 1 that contains novels annotated with XML-TEI tags: TEI-Header, structure and other textual data. See encoding guidelines:\n",
        "\n",
        "https://distantreading.github.io/Training/Budapest/encodingGuide-1.html\n",
        "\n",
        "https://distantreading.github.io/Training/Budapest/encodingGuide-2.html#(1)\n",
        "\n",
        "Level 0 will contain novels in plain texts (currently is empty)\n",
        "\n",
        "Leve 2 will contain novels annotated with Part of Speech and lemmas (soon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz1ozN3wGaM_"
      },
      "source": [
        "## Open each file and extract information about code switching\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS8wt_B-XgA2"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "dir_in = \"/content/ELTeC-spa-master/level1/\" #This directory contains the novels / En este directorio están las novelas\n",
        "\n",
        "foreign_lexicon = {} # Diccionario para almacenar las palabras y su idioma\n",
        "print('Processing', dir_in)\n",
        "\n",
        "for base, directorios, ficheros in os.walk(dir_in): #Recorre el directorio donde están las novelas\n",
        "  for fichero in ficheros:\n",
        "    ficheroEntrada = base + fichero\n",
        "    directorio = base.split('/')[-1]\n",
        "    if fichero[0:3] == \"SPA\": # Language ID / Identificador de idioma.\n",
        "      with open(ficheroEntrada, 'r') as tei: #Opens the file / abre el fichero\n",
        "        soup = BeautifulSoup(tei, 'xml') #Parse the XML / Procesa el XML\n",
        "        #print(\"Processing\", ficheroEntrada) #Only to see the process. Comment if it's not important.\n",
        "        if soup.foreign != None:\n",
        "          foreigns = soup.find_all('foreign') #Extract all tags \"foreign\" / Extra todas las etiquetas \"foreign\", que marcan el cambio de idioma en cada novela. \n",
        "          for item in foreigns: # Por cada etiqueta detectada, extrae el idioma y el fragmento de texto correspondiente. \n",
        "            #print(item)\n",
        "            lang = item[\"xml:lang\"] # Extract the name of the language / Extrae el nombre del idioma\n",
        "            spam = item.text.lower() # Extract the words and tranform to lower case / Extrae las palabras y las pasa a minúsculas\n",
        "            spam = re.sub('\\n*','',spam) # A set of regular expressions to clean fragments. / Esto son expresiones regulares para limipiar el texto: elimina espacios sobrantes, cambios de línea, signos de puntuación, etc.\n",
        "            spam = re.sub('\\t*','',spam)\n",
        "            spam = re.sub('  ','',spam)\n",
        "            spam = re.sub('^ ','',spam)\n",
        "            spam = re.sub(' $','',spam)\n",
        "            spam = re.sub('\\!','',spam)\n",
        "            spam = re.sub('¡','',spam)\n",
        "            spam = re.sub('\\?','',spam)\n",
        "            spam = re.sub('¿','',spam)\n",
        "            spam = re.sub(',','',spam)\n",
        "            spam = re.sub('\\.*','',spam)\n",
        "            spam = re.sub(';','',spam)\n",
        "            spam = re.sub('«','',spam)\n",
        "            spam = re.sub('»','',spam)\n",
        "            spam = re.sub('\\)','',spam)\n",
        "            spam = re.sub('\\(','',spam)\n",
        "            spam = re.sub('^-','',spam)\n",
        "            spam = re.sub('-$','',spam)\n",
        "            spam = re.sub(':','',spam)\n",
        "            \n",
        "            if lang not in foreign_lexicon.keys(): # Create a dictionary: language and its words.\n",
        "             foreign_lexicon[lang] = []            # Crea un diccionario. Idioma: fragmentos de texto en ese idioma.\n",
        "             foreign_lexicon[lang].append(spam)    # Almacena la información en el diccionario\n",
        "            else:\n",
        "              foreign_lexicon[lang].append(spam)   # A partir de ahora, toda la información (idioma y sus textos) está en \"foreign_lexicon\"\n",
        "\n",
        "for item in foreign_lexicon.items():\n",
        "  print(item[0], item[1]) # Show langauges and words / Muestra el resultado: idioma más fragmentos de textos de ese idioma.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReT7op6QN0w3"
      },
      "source": [
        "## Word list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BpTZLbqN0S_"
      },
      "source": [
        "print(\"Generating frequent list\")\n",
        "\n",
        "NumberOfWords = {} # Diccionario para almacenar la cantidad de palabras de cada idioma\n",
        "\n",
        "for item in foreign_lexicon.items(): # Bucle para extraer la información que hay en \"foreign_lexicon\"\n",
        "  lang = item[0] # Guarda el nombre del idioma en la variable \"lang\"\n",
        "  #print(lang)\n",
        "  NumberOfWords[lang] = 0 # To count the number of words for each language / Para contar la cantidad de palabras. Comienza en 0 y luego lo irá incrementando cada vez que cuente una palabra nueva.\n",
        "  \n",
        "  words_out = '' # Variable tipo cadena para almacenar las palabras.\n",
        "  for item in item[1]: # Bucle en los fragmento de textos extrídos de cada idioma.\n",
        "    wrd = item.split(' ') # An extremely simple tokenization / Tokenizador simple para separar las palabas (por espacio en blanco)\n",
        "    for w in wrd:\n",
        "      #print(w)\n",
        "      NumberOfWords[lang]+=1 #Add 1 to the number of words for this langauge / Suma 1 por cada nueva palabra\n",
        "      words_out+=w+'\\n' #Almacena la palabra en \"words_out\"\n",
        "\n",
        "  out = open(lang+'_WordList.txt', 'w') #Opens a file in write mode (\"w\"). / Crea y abre un fichero nuevo (modo escritura) para guardar la lista de palabras de cada idioma.\n",
        "  out.write(words_out) # Escribe las palabras en el fichero\n",
        "  out.close() # Cierra el fichero\n",
        "  # Esos ficheros están en el panel de la izquierda de COLAB, desde donde se pueden descargar.\n",
        "\n",
        "\n",
        "print('Results. Number of words for each language:')\n",
        "results = 'Number of words for each language:\\n'\n",
        "for item in NumberOfWords.items(): # Muestra los resultados finales.\n",
        "  print(item[0], item[1])\n",
        "  results+=item[0]+':\\t'+str(item[1])+'\\n'\n",
        "\n",
        "out_results = open('results.txt', 'w') # Se genera un nuevo fichero para guardar y poder descargar los resultados finales.\n",
        "out_results.write(results)\n",
        "out_results.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-_A4-FtoiM7"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MclHZ84ZF5im"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Toda la información está en \"NumberOfWords\", que se ha generado en el paso anterior.\n",
        "\n",
        "x = NumberOfWords.keys() #Languages / Nombre de cada idioma\n",
        "y = NumberOfWords.values() #Number of words for each lang. / Cantidad de palabras por cada idioma\n",
        "\n",
        "plt.bar(x,y) #Creates the plot\n",
        "plt.xlabel('Language')\n",
        "plt.title('Number of word for each language')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPnAPiuK4HQ"
      },
      "source": [
        "## Show words as WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmUjwfhmCQI3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "#A custom stopwords list: / Lista de \"stopword\" creada \"ad hoc\". Se puede modificar (añadir o quitar palabras) o cargar otra.\n",
        "stopwords = ['por', 'ch', 'ei', 'que','ei', 'io', 'se', 'de', 'est', 'ad', 'et', 'non', 'hoc', 'ex', 'le', 'la', 'qui', 'il', 'di', 'per', 'che', 'les', 'des', 'si', 'un']\n",
        "for item in foreign_lexicon.items():\n",
        "  text = ''\n",
        "  for word in item[1]: # Recorre los fragmentos de texto y vuelve a separarlo en palabras.\n",
        "    for w in word.split(' '):\n",
        "      if w not in stopwords: # Fitro de \"stopwords\"\n",
        "        text+=w+' ' #All words in a string / Si la palabra no está en la lista de \"stopwords\", la guarda en \"text\" como cadena de texto.\n",
        "\n",
        "  wordcloud = WordCloud().generate(text) #Creates the wordcloud, one for each language  / Se genera la nube de palabras, una por cada idioma.\n",
        "\n",
        "  # Display the generated image: / Muestra la nube generada.\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  print(item[0]) #Show the language\n",
        "  plt.show() #Show the cloud\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}