{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELTeC_CodeSwitching.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNe0hq5+GK23BpUgaHlMXI/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bncolorado/Processing-ELTeC-corpus/blob/main/COLAB_notebooks/ELTeC_CodeSwitching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01-XgVe2EIAP"
      },
      "source": [
        "# Extracting code switching\n",
        "\n",
        "One of the TEI tags used in ELTeC corpus is \"foreign\", that marks code switching. This notebook shows how to extract the languages other than Spanish (castilian) in ELTeC-SPA and to generates a word cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCZO3HPMGHDe"
      },
      "source": [
        "## Loading ELTeC-SPA corpus in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyLyrpBnD6v2"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget \"https://github.com/COST-ELTeC/ELTeC-spa/archive/refs/heads/master.zip\" # paste here corpus url\n",
        "\n",
        "zip_ref = zipfile.ZipFile('master.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts files here (/content/)\n",
        "zip_ref.close() \n",
        "!rm master.zip #Removes ZIP to save space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz1ozN3wGaM_"
      },
      "source": [
        "## Open each file and extract information about code switching\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS8wt_B-XgA2"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "dir_in = \"/content/ELTeC-spa-master/level1/\"\n",
        "\n",
        "foreign_lexicon = {}\n",
        "print('Processing', dir_in)\n",
        "\n",
        "for base, directorios, ficheros in os.walk(dir_in):\n",
        "  for fichero in ficheros:\n",
        "    ficheroEntrada = base + fichero\n",
        "    directorio = base.split('/')[-1]\n",
        "    if fichero[0:3] == \"SPA\": # Language ID. Change if you are processing text from ther collection.\n",
        "      with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "        soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "        #print(\"Processing\", ficheroEntrada) #Only to see the process. Comment if it's not important.\n",
        "        if soup.foreign != None:\n",
        "          foreigns = soup.find_all('foreign') #Extract all tags \"foreign\" \n",
        "          for item in foreigns:\n",
        "            #print(item)\n",
        "            lang = item[\"xml:lang\"] # Extract the name of the language\n",
        "            spam = item.text.lower() # Extract the words and tranform to lower case\n",
        "            spam = re.sub('\\n*','',spam) # A set of regular expressions to clean fragments.\n",
        "            spam = re.sub('\\t*','',spam)\n",
        "            spam = re.sub('  ','',spam)\n",
        "            spam = re.sub('^ ','',spam)\n",
        "            spam = re.sub(' $','',spam)\n",
        "            spam = re.sub('\\!','',spam)\n",
        "            spam = re.sub('¡','',spam)\n",
        "            spam = re.sub('\\?','',spam)\n",
        "            spam = re.sub('¿','',spam)\n",
        "            spam = re.sub(',','',spam)\n",
        "            spam = re.sub('\\.*','',spam)\n",
        "            spam = re.sub(';','',spam)\n",
        "            spam = re.sub('«','',spam)\n",
        "            spam = re.sub('»','',spam)\n",
        "            spam = re.sub('\\)','',spam)\n",
        "            spam = re.sub('\\(','',spam)\n",
        "            spam = re.sub('^-','',spam)\n",
        "            spam = re.sub('-$','',spam)\n",
        "            spam = re.sub(':','',spam)\n",
        "            \n",
        "            if lang not in foreign_lexicon.keys(): # Create a dictionary: language and its words.\n",
        "             foreign_lexicon[lang] = []\n",
        "             foreign_lexicon[lang].append(spam)\n",
        "            else:\n",
        "              foreign_lexicon[lang].append(spam)\n",
        "\n",
        "for item in foreign_lexicon.items():\n",
        "  print(item[0], item[1]) # Show langauges and words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReT7op6QN0w3"
      },
      "source": [
        "## Word list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BpTZLbqN0S_"
      },
      "source": [
        "print(\"Generating frequent list\")\n",
        "\n",
        "freqlist = []\n",
        "NumberOfWords = {}\n",
        "\n",
        "for item in foreign_lexicon.items():\n",
        "  lang = item[0]\n",
        "  #print(lang)\n",
        "  NumberOfWords[lang]=0#To count the number of words for each language\n",
        "  out = open(lang+'_WordList.txt', 'w') #Opens a file in write mode (\"w\").\n",
        "  words_out = ''\n",
        "  for item in item[1]:\n",
        "    wrd = item.split(' ') # An extremely simple tokenization\n",
        "    for w in wrd:\n",
        "      #print(w)\n",
        "      NumberOfWords[lang]+=1 #Add 1 to the number of words for this langauge\n",
        "      words_out+=w+'\\n'\n",
        "\n",
        "  out.write(words_out)\n",
        "  out.close()\n",
        "print('Done!')\n",
        "\n",
        "print('Results. Number of words for each language:')\n",
        "results = 'Number of words for each language:\\n'\n",
        "for item in NumberOfWords.items():\n",
        "  print(item[0], item[1])\n",
        "  results+=item[0]+':\\t'+str(item[1])+'\\n'\n",
        "\n",
        "out_results = open('results.txt', 'w')\n",
        "out_results.write(results)\n",
        "out_results.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-_A4-FtoiM7"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MclHZ84ZF5im"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = NumberOfWords.keys() #Languages\n",
        "y = NumberOfWords.values() #Number of words for each lang.\n",
        "\n",
        "plt.bar(x,y) #Creates the plot\n",
        "plt.xlabel('Language')\n",
        "plt.title('Number of word for each language')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qPnAPiuK4HQ"
      },
      "source": [
        "## Show words as WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmUjwfhmCQI3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "#A custom stopwords list:\n",
        "stopwords = ['por', 'ch', 'ei', 'que','ei', 'io', 'se', 'de', 'est', 'ad', 'et', 'non', 'hoc', 'ex', 'le', 'la', 'qui', 'il', 'di', 'per', 'che', 'les', 'des', 'si', 'un']\n",
        "for item in foreign_lexicon.items():\n",
        "  text = ''\n",
        "  for word in item[1]:\n",
        "    for w in word.split(' '):\n",
        "      if w not in stopwords:\n",
        "        text+=w+' ' #All words in a string\n",
        "\n",
        "  wordcloud = WordCloud().generate(text) #Creates the wordcloud, one for each language\n",
        "\n",
        "  # Display the generated image:\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  print(item[0]) #Show the language\n",
        "  plt.show() #Show the cloud\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}